{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import datetime\n",
    "from nab_dataset import NabDataset\n",
    "from models.recurrent_models_pyramid import LSTMGenerator, LSTMDiscriminator\n",
    "from models.soft_dtw_cuda import SoftDTW\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from dtaidistance import dtw\n",
    "from dtaidistance import dtw_visualisation as dtwvis\n",
    "import numpy as np\n",
    "\n",
    "class ArgsTrn:\n",
    "    workers=0\n",
    "    batch_size=32\n",
    "    epochs=20\n",
    "    lr=0.0002\n",
    "    cuda = True\n",
    "    manualSeed=2\n",
    "    window_size=60\n",
    "    \n",
    "opt_trn=ArgsTrn()\n",
    "\n",
    "torch.manual_seed(opt_trn.manualSeed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# location of datasets and category\n",
    "\n",
    "end_name = 'ambient_temperature_system_failure.csv' # dataset name\n",
    "data_file = 'data/realKnownCause/'+end_name # dataset category and dataset name\n",
    "key = 'realKnownCause/'+end_name # This key is used for reading anomaly labels\n",
    "\n",
    "# settings for data loader\n",
    "class DataSettings:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.BASE = 'NAB/'\n",
    "        self.label_file = 'labels/combined_windows.json'\n",
    "        self.data_file = data_file\n",
    "        self.key = key\n",
    "        self.train = True\n",
    "        self.window_length = opt_trn.window_size\n",
    "    \n",
    "    \n",
    "data_settings = DataSettings()\n",
    "\n",
    "# define dataset object and data loader object for NAB dataset\n",
    "dataset = NabDataset(data_settings=data_settings)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt_trn.batch_size,\n",
    "                                         shuffle=True, num_workers=int(opt_trn.workers))\n",
    "\n",
    "dataset.x.shape, dataset.y.shape # check the dataset shape\n",
    "\n",
    "device = torch.device(\"cuda:0\" if opt_trn.cuda else \"cpu\") # select the device\n",
    "seq_len = dataset.window_length # sequence length is equal to the window length\n",
    "in_dim = dataset.n_feature # input dimension is same as number of feature\n",
    "\n",
    "# Create generator and discriminator models\n",
    "netG = LSTMGenerator(in_dim=in_dim, out_dim=in_dim, device=device).to(device)\n",
    "netD = LSTMDiscriminator(in_dim=in_dim, device=device).to(device)\n",
    "\n",
    "print(\"|Generator Architecture|\\n\", netG)\n",
    "print(\"|Discriminator Architecture|\\n\", netD)\n",
    "\n",
    "# Setup loss function\n",
    "criterion = nn.BCELoss().to(device)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt_trn.lr)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt_trn.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Training of Generator and Discriminator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "loss_D = []\n",
    "loss_G = []\n",
    "loss_D_G_z = []\n",
    "\n",
    "for epoch in range(opt_trn.epochs):\n",
    "    for i, (x,y) in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "\n",
    "        #Train with real data\n",
    "        netD.zero_grad()\n",
    "        real = x.to(device)\n",
    "        batch_size, seq_len = real.size(0), real.size(1)\n",
    "        label = torch.full((batch_size, seq_len, 1), real_label, device=device)\n",
    "\n",
    "        output,_ = netD.forward(real)\n",
    "        errD_real = criterion(output, label.float())\n",
    "        errD_real.backward()\n",
    "        optimizerD.step()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        #Train with fake data\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1)).cuda()\n",
    "        fake,_ = netG.forward(noise)\n",
    "        output,_ = netD.forward(fake.detach()) # detach causes gradient is no longer being computed or stored to save memeory\n",
    "        label.fill_(fake_label)\n",
    "        errD_fake = criterion(output, label.float())\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        noise = Variable(init.normal(torch.Tensor(batch_size,seq_len,in_dim),mean=0,std=0.1)).cuda()\n",
    "        fake,_ = netG.forward(noise)\n",
    "        label.fill_(real_label) \n",
    "        output,_ = netD.forward(fake)\n",
    "        errG = criterion(output, label.float())\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "    print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' \n",
    "          % (epoch+1, opt_trn.epochs, i, len(dataloader),\n",
    "             errD.item(), errG.item(), D_x, D_G_z1, D_G_z2), end='')\n",
    "    print()\n",
    "    \n",
    "    # add #\n",
    "    loss_D.append(errD.item())\n",
    "    loss_G.append(errG.item())\n",
    "    mean_D_G_z = (D_G_z1+D_G_z2)/2\n",
    "    loss_D_G_z.append(mean_D_G_z)\n",
    "    \n",
    "    \n",
    "    if epoch > 0:\n",
    "        if (errD.item() <= loss_D[epoch-1]*1.2) & (errG.item() <= loss_G[epoch-1]*1.2) & (errG.item() <= min(loss_G)) & (mean_D_G_z >= max(loss_D_G_z)):\n",
    "            print('-> save model')\n",
    "            torch.save(netD, 'NAB/model/netD_best.pth')\n",
    "            torch.save(netG, 'NAB/model/netG_best.pth')\n",
    "        else:\n",
    "            print('-> Do not save model')\n",
    "    else:\n",
    "        print('-> save model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define basic settings for inverse mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsTest:\n",
    "    workers = 0\n",
    "    batch_size = 1\n",
    "    \n",
    "opt_test=ArgsTest()    \n",
    "\n",
    "# add #\n",
    "netG = torch.load('NAB/model/netD_best.pth')\n",
    "netD = torch.load('NAB/model/netD_best.pth')\n",
    "\n",
    "generator = netG # changing reference variable \n",
    "discriminator = netD # changing reference variable \n",
    "\n",
    "# Define settings for loading data in evaluation mood\n",
    "class TestDataSettings:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.BASE = 'Nab/'\n",
    "        self.label_file = 'labels/combined_windows.json'\n",
    "        self.data_file = data_file\n",
    "        self.key = key\n",
    "        self.train = False\n",
    "        self.window_length = opt_trn.window_size\n",
    "\n",
    "# Lambda = 0.1 according to paper\n",
    "# x is new data, G_z is closely regenerated data\n",
    "\n",
    "def Anomaly_score(x, G_z, Lambda=0.1):\n",
    "    residual_loss = torch.sum(torch.abs(x-G_z)) # Residual Loss\n",
    "    \n",
    "    # x_feature is a rich intermediate feature representation for real data x\n",
    "    output, x_feature = discriminator(x.to(device)) \n",
    "    # G_z_feature is a rich intermediate feature representation for fake data G(z)\n",
    "    output, G_z_feature = discriminator(G_z.to(device)) \n",
    "    \n",
    "    discrimination_loss = torch.sum(torch.abs(x_feature-G_z_feature)) # Discrimination loss\n",
    "    \n",
    "    total_loss = (1-Lambda)*residual_loss.to(device) + Lambda*discrimination_loss\n",
    "    return total_loss\n",
    "\n",
    "def Generator_score(x, G_z, Lambda=0.9):\n",
    "    criterion = nn.MSELoss()\n",
    "    residual_loss = criterion(x, G_z)\n",
    "    \n",
    "    # x_feature is a rich intermediate feature representation for real data x\n",
    "    output, x_feature = discriminator(x.to(device)) \n",
    "    # G_z_feature is a rich intermediate feature representation for fake data G(z)\n",
    "    output, G_z_feature = discriminator(G_z.to(device)) \n",
    "    \n",
    "    discrimination_loss = torch.sum(torch.abs(x_feature-G_z_feature)) # Discrimination loss\n",
    "    total_loss = (1-Lambda)*residual_loss.to(device) + Lambda*discrimination_loss\n",
    "    return total_loss\n",
    "\n",
    "def Similarity_score(x, G_z):\n",
    "    sdtw = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "    loss = sdtw(x, G_z)\n",
    "    return loss.mean()\n",
    "\n",
    "def replaced_minmax(real, z):\n",
    "    path = dtw.warping_path(real, z)\n",
    "    max_value = []\n",
    "    min_value = []\n",
    "    for i in range(len(path)):\n",
    "        if path[i][0] == real.argmax():\n",
    "            max_value.append(z[path[i][1]])\n",
    "            \n",
    "        if path[i][0] == real.argmin():\n",
    "            min_value.append(z[path[i][1]])\n",
    "    real[real.argmax()] = np.mean(max_value)\n",
    "    real[real.argmin()] = np.mean(min_value)\n",
    "    return real\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data_settings = TestDataSettings()\n",
    "\n",
    "# define dataset object and data loader object in evaluation mood for NAB dataset\n",
    "test_dataset = NabDataset(test_data_settings)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=opt_test.batch_size, \n",
    "                                         shuffle=False, num_workers=int(opt_test.workers))\n",
    "\n",
    "test_dataset.x.shape, test_dataset.y.shape, test_dataset.data_len # check the dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Anomaly detection for real data\n",
    "step1_z_list = []\n",
    "step1_gen_list = []\n",
    "step1_real_list = []\n",
    "step1_loss_list = []\n",
    "\n",
    "for i, (x,y) in enumerate(test_dataloader):\n",
    "    z = Variable(init.normal(torch.zeros(opt_test.batch_size,\n",
    "                                     test_dataset.window_length, \n",
    "                                     test_dataset.n_feature),mean=0,std=0.1),requires_grad=True)\n",
    "    #z = x\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-2)\n",
    "    \n",
    "    loss = None\n",
    "    for j in range(50): # set your interation range\n",
    "        gen_fake,_ = generator(z.cuda())\n",
    "        loss = Anomaly_score(Variable(x).cuda(), gen_fake)\n",
    "        loss.backward()\n",
    "        z_optimizer.step()\n",
    "    \n",
    "    # add    \n",
    "    step1_z_list.append(z)\n",
    "    step1_gen_list.append(gen_fake)\n",
    "    step1_real_list.append(Variable(x).cuda())\n",
    "    step1_loss_list.append(loss) # Store the loss from the final iteration\n",
    "    \n",
    "    print('Index :', i, 'Anomaly :', y, 'loss={}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualise Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5.9 \n",
    "THRESHOLD = threshold\n",
    "\n",
    "#TIME_STEPS = dataset.window_length\n",
    "test_score_df = pd.DataFrame(index=range(test_dataset.data_len))\n",
    "test_score_df['loss'] = [loss.item()/test_dataset.window_length for loss in step1_loss_list]\n",
    "test_score_df['y'] = test_dataset.y\n",
    "test_score_df['threshold'] = THRESHOLD\n",
    "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "test_score_df['t'] = [x[ArgsTrn.window_size-1].item() for x in test_dataset.x] \n",
    "\n",
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss', color='k')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold', color='r')\n",
    "plt.plot(test_score_df.index, test_score_df.y, label='anomaly', color='orange')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend();\n",
    "plt.ylim([-1,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomalies = test_score_df[test_score_df.anomaly == True]\n",
    "\n",
    "plt.plot(\n",
    "  range(test_dataset.data_len), \n",
    "  test_score_df['t'], \n",
    "  label='value',\n",
    "  color='black'\n",
    ");\n",
    "\n",
    "sns.scatterplot(\n",
    "  # anomalies.index,\n",
    "  anomalies.t,\n",
    "  color=sns.color_palette()[3],\n",
    "  s=52,\n",
    "  label='predict_anomaly'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "  range(len(test_score_df['y'])),\n",
    "  test_score_df['y'],\n",
    "  label='anomaly',\n",
    "  color = 'orange'\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend();\n",
    "plt.ylim([-4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the window-based anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end = []\n",
    "state = 0\n",
    "for idx in test_score_df.index:\n",
    "    if state==0 and test_score_df.loc[idx, 'y']==1:\n",
    "        state=1\n",
    "        start = idx\n",
    "    if state==1 and test_score_df.loc[idx, 'y']==0:\n",
    "        state = 0\n",
    "        end = idx\n",
    "        start_end.append((start, end))\n",
    "\n",
    "for s_e in start_end:\n",
    "    if sum(test_score_df[s_e[0]:s_e[1]+1]['anomaly'])>0:\n",
    "        for i in range(s_e[0], s_e[1]+1):\n",
    "            test_score_df.loc[i, 'anomaly'] = 1\n",
    "            \n",
    "actual = np.array(test_score_df['y'])\n",
    "predicted = np.array([int(a) for a in test_score_df['anomaly']])\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "predicted = np.array(predicted)\n",
    "actual = np.array(actual)\n",
    "\n",
    "tp = np.count_nonzero(predicted * actual)\n",
    "tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
    "fp = np.count_nonzero(predicted * (actual - 1))\n",
    "fn = np.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "print('True Positive\\t', tp)\n",
    "print('True Negative\\t', tn)\n",
    "print('False Positive\\t', fp)\n",
    "print('False Negative\\t', fn)\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
    "auc_val = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc_val = roc_auc_score(actual, predicted)\n",
    "\n",
    "print('Accuracy\\t', accuracy)\n",
    "print('Precision\\t', precision)\n",
    "print('Recall\\t', recall)\n",
    "print('f-measure\\t', fmeasure)\n",
    "print('cohen_kappa_score\\t', cohen_kappa_score)\n",
    "print('auc\\t', auc_val)\n",
    "print('roc_auc\\t', roc_auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = test_score_df[test_score_df.anomaly == True]\n",
    "\n",
    "plt.plot(\n",
    "  range(test_dataset.data_len), \n",
    "  test_score_df['t'], \n",
    "  label='value',\n",
    "  color='black'\n",
    ");\n",
    "\n",
    "sns.scatterplot(\n",
    "  # anomalies.index,\n",
    "  anomalies.t,\n",
    "  color=sns.color_palette()[3],\n",
    "  s=52,\n",
    "  label='predict_anomaly'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "  range(len(test_score_df['y'])),\n",
    "  test_score_df['y'],\n",
    "  label='anomaly',\n",
    "  color = 'orange'\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend();\n",
    "plt.ylim([-4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the anomaly index \n",
    "raw_anomaly_index = []\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i] == 1:\n",
    "        # print(i, predicted[i])\n",
    "        raw_anomaly_index.append(i)\n",
    "raw_anomaly_index = np.array(raw_anomaly_index)\n",
    "raw_anomaly_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genenrate entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_z_list = []\n",
    "step2_gen_list = []\n",
    "step2_real_list = []\n",
    "step2_loss_list = []\n",
    "\n",
    "total_mean = []\n",
    "total_std = []\n",
    "\n",
    "for i, (x,y) in enumerate(test_dataloader):\n",
    "\n",
    "    win_mean = np.mean(Variable(x)[-1,:,-1].cpu().detach().numpy())\n",
    "    win_std = np.std(Variable(x)[-1,:,-1].cpu().detach().numpy())\n",
    "    \n",
    "    total_mean.append(win_mean) \n",
    "    total_std.append(win_std)\n",
    "    \n",
    "    z = Variable(init.normal(torch.zeros(opt_test.batch_size,\n",
    "                                     test_dataset.window_length, \n",
    "                                     test_dataset.n_feature),\n",
    "                                     mean=win_mean,std=win_std),requires_grad=True)\n",
    "\n",
    "    z_optimizer = torch.optim.Adam([z],lr=5e-3) # 1e-2, 5e-3\n",
    "    \n",
    "    loss = None\n",
    "    \n",
    "    for j in range(50): # set your interation range\n",
    "        gen_fake,_ = generator(z.cuda())\n",
    "        loss = Generator_score(Variable(x).cuda(), gen_fake) + Similarity_score(Variable(x).cuda(), z.cuda())\n",
    "        loss.backward()\n",
    "        z_optimizer.step()\n",
    "\n",
    "    # add    \n",
    "    step2_z_list.append(z)\n",
    "    step2_gen_list.append(gen_fake)\n",
    "    step2_real_list.append(Variable(x).cuda())\n",
    "    step2_loss_list.append(loss) # Store the loss from the final iteration\n",
    "\n",
    "    print('Generate process - Index :', i, 'Anomaly :', y, 'loss={}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = NabDataset(test_data_settings)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=opt_test.batch_size, \n",
    "                                         shuffle=False, num_workers=int(opt_test.workers))\n",
    "\n",
    "replace_dataset = NabDataset(test_data_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_idx = np.array(list(range(replace_dataset.x.shape[0])))\n",
    "normal_idx = np.delete(total_idx, [raw_anomaly_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Step 1. Anomaly detection for total data\n",
    "###################################################################\n",
    "from datetime import datetime\n",
    "\n",
    "ano_idx = raw_anomaly_index\n",
    "epochs = 30\n",
    "\n",
    "step3_z_list = step2_z_list\n",
    "step3_gen_list = step2_gen_list\n",
    "step3_real_list = step2_real_list\n",
    "step3_loss_list = step2_loss_list\n",
    "\n",
    "mean_window_raw = np.zeros(shape=(len(replace_dataset),))\n",
    "std_window_raw = np.zeros(shape=(len(replace_dataset),))\n",
    "for i in range(epochs):\n",
    "    epoch = i\n",
    "    start_time = datetime.now()\n",
    "    print('Replacement process - epoch: {}/{} Start time: {}'.format(i+1, epochs, start_time))\n",
    "    replace_dataloader = torch.utils.data.DataLoader(replace_dataset, batch_size=opt_test.batch_size, \n",
    "                                         shuffle=False, num_workers=int(opt_test.workers))\n",
    "    \n",
    "    ###################################################################\n",
    "    # Step 2. Generate data\n",
    "    ###################################################################\n",
    "    for i, (x,y) in enumerate(replace_dataloader):\n",
    "        # generate about only anomaly data\n",
    "        if i in ano_idx:\n",
    "            # define mean and std\n",
    "            mean_window = np.mean(Variable(x)[-1,:,-1].cpu().detach().numpy())\n",
    "            std_window = np.std(Variable(x)[-1,:,-1].cpu().detach().numpy())\n",
    "            \n",
    "            # 초기 평균 및 표준편차 저장\n",
    "            if epoch == 0:\n",
    "                mean_window_raw[i] = mean_window\n",
    "                std_window_raw[i] = std_window\n",
    "                \n",
    "            if epoch % 5 == 0:\n",
    "                mean_window = mean_window_raw[i]\n",
    "                std_window = std_window_raw[i]      \n",
    "            \n",
    "            nm_idx = find_nearest(normal_idx, i)\n",
    "            new_mean = total_mean[nm_idx]\n",
    "            new_std = total_std[nm_idx]/2\n",
    "            \n",
    "            z = Variable(init.normal_(torch.zeros(opt_test.batch_size,\n",
    "                                                  replace_dataset.window_length, \n",
    "                                                  replace_dataset.n_feature),\n",
    "                                                  # mean=mean_window,std=std_window),requires_grad=True)\n",
    "                                                  # mean=total_mean[nm_idx],std=total_std[nm_idx]),requires_grad=True)\n",
    "                                                  mean=new_mean, std=new_std), requires_grad=True)\n",
    "\n",
    "            z_optimizer = torch.optim.Adam([z],lr=5e-3) # 1e-2, 5e-3\n",
    "\n",
    "            loss = None\n",
    "            for j in range(100): # set your interation range\n",
    "                gen_fake,_ = generator(z.cuda())\n",
    "                loss = Anomaly_score(Variable(x).cuda(), gen_fake) + Similarity_score(step2_z_list[nm_idx].cuda(), z.cuda()) + Similarity_score(Variable(x).cuda(), z.cuda())\n",
    "                loss.mean().backward()\n",
    "                z_optimizer.step()\n",
    "\n",
    "            # add        \n",
    "            step3_z_list[i] = z\n",
    "            step3_gen_list[i] = gen_fake\n",
    "            step3_real_list[i] = Variable(x).cuda()\n",
    "            step3_loss_list[i] = loss # Store the loss from the final iteration\n",
    "\n",
    "            # print('Generate process - Index :', i, 'Anomaly :', y, 'loss={}'.format(loss))\n",
    "            \n",
    "    ###################################################################\n",
    "    # Step 3. Replace real data with generated data\n",
    "    ###################################################################\n",
    "    ## step 1. replace generate data with real data\n",
    "    replaced_data = []\n",
    "    for i in ano_idx:\n",
    "        real = step3_real_list[i][-1,:,-1].cpu().detach().numpy()\n",
    "        z = step3_z_list[i][-1,:,-1].cpu().detach().numpy()\n",
    "        replaced_data.append(replaced_minmax(real, z))\n",
    "\n",
    "    ## step 2. mapping replaced data to real data\n",
    "    for i, j in enumerate(ano_idx):\n",
    "        replace_dataset.x[j] = torch.from_numpy(replaced_data[i].reshape(60,1))\n",
    "        \n",
    "    ###################################################################\n",
    "    # Step 4. Anomaly detection for anomaly data\n",
    "    ###################################################################\n",
    "    step4_loss_list = step1_loss_list\n",
    "    for i, (x,y) in enumerate(replace_dataloader):\n",
    "        if i in ano_idx:\n",
    "            z = Variable(init.normal_(torch.zeros(opt_test.batch_size,\n",
    "                                             test_dataset.window_length, \n",
    "                                             test_dataset.n_feature),mean=0,std=0.1),requires_grad=True)\n",
    "            #z = x\n",
    "            z_optimizer = torch.optim.Adam([z],lr=1e-2)\n",
    "\n",
    "            loss = None\n",
    "            for j in range(50): # set your interation range\n",
    "                gen_fake,_ = generator(z.cuda())\n",
    "                loss = Anomaly_score(Variable(x).cuda(), gen_fake)\n",
    "                loss.backward()\n",
    "                z_optimizer.step()\n",
    "\n",
    "            step4_loss_list[i] = loss # Store the loss from the final iteration\n",
    "        \n",
    "    THRESHOLD = threshold # Anomaly score threshold for an instance to be considered as anomaly \n",
    "\n",
    "    #TIME_STEPS = dataset.window_length\n",
    "    test_score_df_after = pd.DataFrame(index=range(replace_dataset.data_len))\n",
    "    test_score_df_after['loss'] = [loss.item()/replace_dataset.window_length for loss in step4_loss_list]\n",
    "    test_score_df_after['y'] = replace_dataset.y\n",
    "    test_score_df_after['threshold'] = THRESHOLD\n",
    "    test_score_df_after['anomaly'] = test_score_df_after.loss > test_score_df_after.threshold\n",
    "    test_score_df_after['t'] = [x[ArgsTrn.window_size-1].item() for x in replace_dataset.x]\n",
    "    \n",
    "    start_end = []\n",
    "    state = 0\n",
    "    for idx in test_score_df_after.index:\n",
    "        if state==0 and test_score_df_after.loc[idx, 'y']==1:\n",
    "            state=1\n",
    "            start = idx\n",
    "        if state==1 and test_score_df_after.loc[idx, 'y']==0:\n",
    "            state = 0\n",
    "            end = idx\n",
    "            start_end.append((start, end))\n",
    "\n",
    "    for s_e in start_end:\n",
    "        if sum(test_score_df_after[s_e[0]:s_e[1]+1]['anomaly'])>0:\n",
    "            for i in range(s_e[0], s_e[1]+1):\n",
    "                test_score_df_after.loc[i, 'anomaly'] = 1\n",
    "\n",
    "    actual = np.array(test_score_df_after['y'])\n",
    "    predicted = np.array([int(a) for a in test_score_df_after['anomaly']])\n",
    "   \n",
    "    # define the anomaly index \n",
    "    ano_idx = []\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == 1:\n",
    "            ano_idx.append(i)\n",
    "    ano_idx = np.array(ano_idx)\n",
    "    ano_idx\n",
    "    \n",
    "    if len(ano_idx) < 1:\n",
    "        print('Finish Replacement process')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "gan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
